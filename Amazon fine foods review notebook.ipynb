{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Technique exploration notebook #\n",
    "\n",
    "This notebook is part of my journey into Machine Learning, the idea is to investigate and learn about the various different techniques of Natural Language Processing, then try to apply them to a dataset and find out what we can learn from it. The dataset I'll use consists of about 500,000 reviews of fine foods from Amazon"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-3084c2bff013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
>>>>>>> parent of 81d27ba... sklearn.metrics bug
   "source": [
    "#Importing pandas which is great tool to handle data easily\n",
    "\n",
    "import pandas as pd\n",
    "#Analisis de datos\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "#Pre-procesamiento de datos\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics\n",
    "import gensim \n",
    "import scipy\n",
    "\n",
    "# Modelos predictivos\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import keras\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First things first#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by analyzing the data we have, see what are the important attributes, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"Amazon reviews/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 568454 reviews that have 10 features each! <br />\n",
    "Let's see what the reviews look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the most relevant features are: <br />\n",
    "    -The text feature that contains the review itself <br />\n",
    "    -The Summary of the review\n",
    "    -The score the author of the review gave the product, which is a number ranging from 1 to 5 <br />\n",
    "    -The feature HelpfulnessNumerator which means how many people found the review usefull <br />\n",
    "    -The feature HelpfulnessDenominator which means how many people indicated whether the review was helpfull or not    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that i'm wondering is what the score value distribution looks like, so lets check that out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 0\n",
    "for i in range(len(train['Text'])):\n",
    "    if train['Score'][i]==1:\n",
    "           var+=1\n",
    "            \n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "            x = [5,4,1,3,2],\n",
    "            y = train.Score.value_counts().values,\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = train.Score.value_counts().values\n",
    "                        ),\n",
    "            text='Reviews'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Score distribution'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')\n",
    "#py.plot(fig, filename='barplot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are a lot more 5 score reviews than the rest, it is important to use balanced data when working with classification problems, so later we'll probably have to select a subset of this dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could knowing the review's length distribution be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = train[train['Score'] == 1]['Text'].str.len()\n",
    "sns.distplot(s1,color=\"red\", label='Puntaje 1')\n",
    "s2 =  train[train['Score'] == 2]['Text'].str.len()\n",
    "sns.distplot(s2,color=\"skyblue\", label='Puntaje 2')\n",
    "s3 = train[train['Score'] == 3]['Text'].str.len()\n",
    "sns.distplot(s3,color=\"purple\", label='Puntaje 3')\n",
    "s4 = train[train['Score'] == 4]['Text'].str.len()\n",
    "sns.distplot(s4,color=\"green\" ,label='Puntaje 4')\n",
    "s5 = train[train['Score'] == 5]['Text'].str.len()\n",
    "sns.distplot(s5,color=\"orange\", label='Puntaje 5')\n",
    "plt.title(\"Review's length distribution\")\n",
    "#sns.plt.ylim(0, 1000)\n",
    "plt.xlim(0, 2000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORDCLOUDS\n",
    "Wordclouds are a great way to quickly see what the general sentiment of the reviews are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We make all words lowercase, then separate each word in the reviews\n",
    "import re\n",
    "import string\n",
    "cleanup_re = re.compile('[^a-z]+')\n",
    "def cleanup(sentence):\n",
    "    if type(sentence) is str:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = cleanup_re.sub(' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "train[\"Summary_Clean\"] = train[\"Summary\"].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the reviews with the highest scores and the ones with the lowest\n",
    "star1= train[train.Score==1][\"Summary_Clean\"].values\n",
    "star5= train[train.Score==5][\"Summary_Clean\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use pictures as an outline for the wordcloud\n",
    "import codecs\n",
    "from PIL import Image\n",
    "img = Image.open(\"Masks/hamburger2.jpg\")\n",
    "hcmask = np.array(img)\n",
    "img = Image.open(\"Masks/apple2.jpg\")\n",
    "hcmask2= np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate the 1 score wordcloud\n",
    "plt.figure(figsize=[20,10])\n",
    "wc=WordCloud(width=1200, height=600,background_color='black',mask=hcmask,max_words=2000,stopwords=STOPWORDS,max_font_size=120)\n",
    "wc.generate(\" \".join(str(v) for v in star1))\n",
    "plt.title(\"1 Score Reviews\", fontsize=20)\n",
    "plt.imshow(wc.recolor(colormap='hot',random_state=17), alpha=0.98)\n",
    "plt.axis('off')\n",
    "#savefig('bad_food_wordcloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the 5 score wordcloud\n",
    "plt.figure(figsize=[20,10])\n",
    "wc=WordCloud(width=1200, height=600, background_color='black',mask=hcmask2,max_words=2000,stopwords=STOPWORDS,max_font_size=80)\n",
    "wc.generate(\" \".join(str(v) for v in star5))\n",
    "plt.title(\"5 Score Reviews\", fontsize=20)\n",
    "plt.imshow(wc.recolor(colormap='inferno',random_state=17), alpha=0.98)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation \n",
    "Latent dirichlet allocation is a stadistic model, that uses unsupervised learning to form clusters of samples that are related to each other. In the context of NLP, this technique will make groups of related words that we will call topics, and that then means that each review is formed by a distribution of topics. <br />\n",
    "An example: <br />\n",
    "We could have <br />\n",
    "topic 1: tasty good delicious, amazing, great <br />\n",
    "topic 2: sugar, candy, tasty, dessert <br />\n",
    "So then we could have a review that is 40% topic 1 and 60% topic B <br />\n",
    "This can be useful to determine which reviews are similar to each other without having to read them all <br /> \n",
    "As this is unsupervised learning, we are the ones that have to decide what the meaning of the topics are, the model will just give us topic 1, 2, 3 etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I took 3000 samples of the reviews with score 1 and 3000 of reviews with score 5\n",
    "#Then I made a matrix formed by the bag of words vector of each review\n",
    "star1_text= train[train.Score==1][\"Text\"].values\n",
    "star5_text= train[train.Score==5][\"Text\"].values\n",
    "star1_text_sample= star1_text[:3000].copy()\n",
    "star5_text_sample= star5_text[:3000].copy()\n",
    "\n",
    "countvec_lda= LemmaCountVectorizer(max_df=0.95,min_df=5,stop_words=\"english\",decode_error='ignore')\n",
    "X_train_counts_lda_neg = countvec_lda.fit_transform(star1_text_sample)   # Matriz bag of words de los datos de entrenamiento\n",
    "\n",
    "print(X_train_counts_lda_neg.shape)\n",
    "print(X_train_counts_lda_neg.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have 3000 reviews, and we have 2775 unique words. The vectors wil have a 1  if the word appears in the review and a 0 if it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I instantiate the LDA model with the score 1 reviews and train it, I decided to generate 14 topics\n",
    "lda = LatentDirichletAllocation(n_components=14,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0,\n",
    "                                n_jobs=-1)\n",
    "lda_Z= lda.fit_transform(X_train_counts_lda_neg)\n",
    "\n",
    "print(lda_Z.shape,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topics are composed by words, with each word having a different weight, so let's see the top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)\n",
    "\n",
    "n_top_words = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use wordclouds again to see what the topics look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic_neg = lda.components_[3]\n",
    "second_topic_neg = lda.components_[8]\n",
    "#third_topic_neg = lda.components_[2]\n",
    "#fourth_topic = lda.components_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "first_topic_words_neg = [tf_feature_names[i] for i in first_topic_neg.argsort()[:-50 - 1 :-1]]\n",
    "second_topic_words_neg = [tf_feature_names[i] for i in second_topic_neg.argsort()[:-50 - 1 :-1]]\n",
    "#third_topic_words_neg = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\n",
    "#fourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the wordcloud\n",
    "def wordcloud_lda(topic_words):\n",
    "    cloud = WordCloud(   stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=3500,\n",
    "                          max_font_size=750,\n",
    "                          height=2400,\n",
    "                         ).generate(\" \".join(topic_words))\n",
    "    plt.figure(figsize=[11,5.5])\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts_lda_pos = countvec_lda.fit_transform(star5_text_sample)\n",
    "lda_Z= lda.fit_transform(X_train_counts_lda_pos)\n",
    "tf_feature_names = countvec_lda.get_feature_names()\n",
    "print(\"Topics generated from 5 score reviews\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic_pos = lda.components_[1]\n",
    "second_topic_pos = lda.components_[12]\n",
    "#third_topic_pos = lda.components_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "first_topic_words_pos = [tf_feature_names[i] for i in first_topic_pos.argsort()[:-50 - 1 :-1]]\n",
    "second_topic_words_pos = [tf_feature_names[i] for i in second_topic_pos.argsort()[:-50 - 1 :-1]]\n",
    "#third_topic_words_neg = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\n",
    "#fourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_lda(first_topic_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_lda(second_topic_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_lda(first_topic_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_lda(second_topic_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contractions\n",
    "# found at http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing irrelevant characters\n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"br\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"href\", \"\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    # Reemplazo contractions\n",
    "    dflist=df[text_field].tolist()\n",
    "    for i in range(len(dflist)):\n",
    "        if True:\n",
    "            dflist[i] = dflist[i].split()\n",
    "            new_text = []\n",
    "            for word in dflist[i]:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            dflist[i] = \" \".join(new_text)\n",
    "            \n",
    "    \n",
    "    return dflist\n",
    "\n",
    "train_list = standardize_text(train, \"Text\")\n",
    "\n",
    "train.to_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here's an example of a clean review\")\n",
    "print(train_list[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have so many reviews and for this purpose we don't need to get the best results possible, we can select a subset of the data in a way that there is a balanced proportion of score values, meaning we want to have 20% 1 score reviews, 20% 2 score reviews and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this kind of classification problem, what really interest us is to try to predict if a review is positive or negative, so we can then analyse a big amount of reviews without having to read them\n",
    "In this case I decided to divide the reviews between positive, negative and neutral like this: <br />\n",
    "Score 4 and 5 is a positive review <br />\n",
    "Score 3 is a neutral review <br />\n",
    "Score 1 and 2 is a negative review <br />\n",
    "Let's code that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the feature Sentiment, which can be positive negative or neutral\n",
    "train[\"Sentiment\"] = train[\"Score\"].apply(lambda score: \"positive\" if score > 3 else (\"negative\" if score <3 else \"neutral\"))\n",
    "#Then we convert this into numerical values  NEGATIVE=0 POSITIVE= 1 NEUTRAL = 2\n",
    "train[\"SentimentB\"] = train[\"Score\"].apply(lambda score: 1 if score > 3 else (0 if score <3 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Helpful %'] = np.where(train['HelpfulnessDenominator'] > 0, train['HelpfulnessNumerator'] / train['HelpfulnessDenominator'], -1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns={'HelpfulnessNumerator': 'PosVotes'}, inplace=True)\n",
    "train['NegVotes'] = (train['HelpfulnessDenominator']-train['PosVotes'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start the classifying problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto=train.Text.tolist() # This will be X\n",
    "texto= train_list\n",
    "scores=train.SentimentB.tolist() # This will be  Y\n",
    "print(texto[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I found this function that allows you to select from the dataset a certain amount of reviews with each score\n",
    "#In this case I decided to take 30.000 reviews for every sentiment value, meaning 90.000 reviews total\n",
    "def balanced_sample_maker(X, y, sample_size, random_seed=None):\n",
    "    \"\"\" return a balanced data set by sampling all classes with sample_size \n",
    "        current version is developed on assumption that the positive\n",
    "        class is the minority.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    X: {numpy.ndarrray}\n",
    "    y: {numpy.ndarray}\n",
    "    \"\"\"\n",
    "    uniq_levels = np.unique(y)\n",
    "    uniq_counts = {level: sum(y == level) for level in uniq_levels}\n",
    "\n",
    "    if not random_seed is None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # find observation index of each class levels\n",
    "    groupby_levels = {}\n",
    "    for ii, level in enumerate(uniq_levels):\n",
    "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "        groupby_levels[level] = obs_idx\n",
    "    # oversampling on observations of each label\n",
    "    balanced_copy_idx = []\n",
    "    for gb_level, gb_idx in groupby_levels.items():\n",
    "        over_sample_idx = np.random.choice(gb_idx, size=sample_size, replace=True).tolist()\n",
    "        balanced_copy_idx+=over_sample_idx\n",
    "    np.random.shuffle(balanced_copy_idx)\n",
    "    return (balanced_copy_idx)\n",
    "\n",
    "index_b = balanced_sample_maker(texto,scores, int((30000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(index_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbalanced= []\n",
    "ybalanced= []\n",
    "for i in range(len(index_b)):\n",
    "    xbalanced.append(texto[index_b[i]])\n",
    "    ybalanced.append(scores[index_b[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ybalanced.count(0))\n",
    "print(ybalanced.count(1))\n",
    "print(ybalanced.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xbalanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model and test the predictions, I divide the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and validation sets\n",
    "# Stratify is used so the training  and testing sets have the same proportion of sentiment values as the original set\n",
    "#This means if there are 50% positive reviews, 20% negative and  y 30% neutral, The same proportion will exist in the \n",
    "#training and validation set\n",
    "#Random state is the seed for the random number generator, it allows you to replicate results if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(xbalanced, ybalanced,stratify=ybalanced, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciating Count Vectorizer\n",
    "countvec= LemmaCountVectorizer(max_df=0.95,min_df=5,stop_words=\"english\",decode_error='ignore') #agregar ngram?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting review's list into bag of words\n",
    "X_train_counts = countvec.fit_transform(X_train)   # Matriz bag of words de los datos de entrenamiento\n",
    "X_test_counts = countvec.transform(X_test)         # Matriz bag of words de datos de testeo\n",
    "\n",
    "print(\"Shape de la matriz BOW:\",X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analisis Graph\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=3)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['orange','blue','green']\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            red_patch = mpatches.Patch(color='orange', label='Negative')\n",
    "            green_patch = mpatches.Patch(color='blue', label='Positive')\n",
    "            blue_patch = mpatches.Patch(color='green',label=\"Neutral\")\n",
    "            plt.legend(handles=[red_patch, green_patch,blue_patch], prop={'size': 30})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this graph, it doesn't seem like it would be easy to differentiate between the diferent types of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can start with a simple logistic regression model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "regstrength=[]\n",
    "for i in range(10):\n",
    "    regstrength.append(30.0)\n",
    "clf = LogisticRegressionCV(Cs=regstrength, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "clf.fit(X_train_counts, y_train)  #I train the model with the bag of words vector of each review and their scores\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 53,
>>>>>>> parent of 81d27ba... sklearn.metrics bug
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1d41f820510b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\54221\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrier_score_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madjusted_mutual_info_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\54221\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\cluster\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfowlkes_mallows_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalinski_harabaz_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\54221\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\54221\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpairwise_fast\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_chi2_kernel_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sparse_manhattan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__file__)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-58f9c2b3e0c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predicted_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_metrics' is not defined"
     ]
    }
   ],
>>>>>>> parent of 81d27ba... sklearn.metrics bug
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a confusion matrix so we can see in which cases the model is making wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_counts)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Negative','Positive','Neutral'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have expected, the model seems to have an easier time differentiating between positive and negative reviews, but has a harder time with neutral reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how the model does with the training set. This could be useful to tell if we are overfitting, in which case the metrics would be way higher than for the validating set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_counts_train = clf.predict(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_train, y_predicted_counts_train)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematization\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "class LemmaTFidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaTFidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instancio el vectorizador TF-IDF\n",
    "tfidfvec= LemmaTFidfVectorizer(max_df=0.95,min_df=5, stop_words='english',decode_error='ignore',strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierto la lista de reviews en Bag of words\n",
    "X_train_tfidf = tfidfvec.fit_transform(X_train)   # Matriz bag of words de los datos de entrenamiento\n",
    "X_test_tfidf = tfidfvec.transform(X_test)         # Matriz bag of words de datos de testeo\n",
    "\n",
    "print(\"Shape de la matriz BOW:\",X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analisis graph\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't seem to help much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### vectorizer coefficient's:\n",
    "\n",
    "**It indicates the degree of influence that each word has in the model's prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " words = tfidfvec.get_feature_names()\n",
    " feature_coefs = pd.DataFrame(\n",
    "     data = list(zip(words, clf.coef_[1])),\n",
    "     columns = ['feature', 'coef'])\n",
    "\n",
    " feature_coefs.sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegressionCV(Cs=regstrength, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "clf2.fit(X_train_tfidf, y_train)  #Entreno el modelo con el bag of words de cada review y los puntajes\n",
    "\n",
    "y_predicted_tfidf = clf2.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_tfidf)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer's coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = tfidfvec.get_feature_names()\n",
    "# feature_coefs = pd.DataFrame(\n",
    "#     data = list(zip(words, clf2.coef_[1])),\n",
    "#     columns = ['feature', 'coef'])\n",
    "\n",
    "# feature_coefs.sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with count vectorizer\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective=\"multi:softprob\", eta=0.1,max_depth=20, n_estimators=150, colsample_bytree=0.4,colsample_bylevel=1, \n",
    "                        subsample=0.6, nthread=10,reg_lambda=10, learning_rate=0.2,min_child_weight=10,gamma=0,n_jobs=4)\n",
    "dtrain = xgb.DMatrix(X_train_tfidf, label=y_train, feature_names=tfidfvec.get_feature_names())\n",
    "xgb_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_xgb = xgb_clf.predict(X_test_tfidf)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_xgb)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check here which words influenced the model the most when making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(xgb_clf.feature_importances_, tfidfvec.get_feature_names()).sort_values(ascending=False).nlargest(25)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes a lot more sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += (os.pathsep + \"C:/Program Files (x86)/Graphviz2.38/bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can check the XGBoost tree\n",
    "xgb.plot_tree(xgb_clf, num_trees=3)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "fig.savefig('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, it seems like the word2vec model works better if we don't eliminate stopwords, so I left them in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train= pd.read_csv(\"Amazon reviews/Reviews.csv\") \n",
    "# texto_noSW=train.Text.tolist()\n",
    "# index_b_noSW = balanced_sample_maker(texto_noSW,puntajes, int((0.05*len(texto_noSW))))\n",
    "# xbalanced_noSW= []\n",
    "# ybalanced_noSW= []\n",
    "# for i in range(len(index_b_noSW)):\n",
    "#     xbalanced_noSW.append(texto[index_b_noSW[i]])\n",
    "#     ybalanced_noSW.append(puntajes[index_b_noSW[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ybalanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we tokenize the reviews, meaning we split the reviews into lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_w2v=[]\n",
    "print(\"Reviews matrix\"xbalanced[:1])\n",
    "for i in range(len(xbalanced)):\n",
    "    texto_w2v.append(gensim.utils.simple_preprocess(xbalanced[i]))\n",
    "print(\"Tokenized sentence matrix:\",texto_w2v[:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xbalanced))\n",
    "print(len(texto_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I generate the vocabulary and train the model\n",
    "#Relevant parameters for the word2vec model:\n",
    "# Size it corresponds to the size or dimensionality of the feature vector\n",
    "# Window: The size of the context window, how many words ahead and backwards the model takes into account\n",
    "#min_count: words that have a frecuency that is less than this value will be ignored\n",
    "modelo = gensim.models.Word2Vec(\n",
    "        texto_w2v,\n",
    "        sg=1,\n",
    "        size=150,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4,\n",
    "         hs=1,\n",
    "        negative=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what relations our model learned with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo.wv.most_similar('tasty', topn=3))\n",
    "print(modelo.wv.most_similar('terrible', topn=3))\n",
    "print(modelo.wv.most_similar('love', topn=3))\n",
    "print(modelo.wv.most_similar('food', topn=3))\n",
    "print(modelo.wv.most_similar('taste', topn=3))\n",
    "print(modelo.wv.most_similar('garbage', topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty cool! Next I will use this word vectors to make reviews vectors, and use them to train some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors= modelo.wv.vectors\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 46510 unique words, each one represented by a 150 dimension vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the average of the word vectors in a review to represent it\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 10000th review\n",
    "        if counter%10000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs= getAvgFeatureVecs(texto_w2v,modelo,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs2= np.array(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analisis graph\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(trainDataVecs, ybalanced)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see the difference between the classes a lot more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(trainDataVecs, ybalanced,stratify=ybalanced, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_clf_w2v= clf2.predict(X_test_w2v)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test_w2v, y_predicted_clf_w2v)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the logistic regression model did worse, that's interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_xgb_w2v= xgb_clf.predict(X_test_w2v)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test_w2v, y_predicted_xgb_w2v)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big improvement with this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_xgb_w2v)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Negative','Positive','Neutral'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Word2Vec y LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec generates word embeddings, which we used to generate review embeddings. At the same time, LDA generates review embeddings, corresponding to their topic distribution, so we can try combining this embeddings and check if that turns into a better representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciating LDA\n",
    "lda = LatentDirichletAllocation(n_components=12,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0,\n",
    "                                n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec_lda2= LemmaCountVectorizer(max_df=0.95,min_df=5,stop_words=\"english\",decode_error='ignore') #agregar ngram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizers\n",
    "vec_bow= countvec_lda2.fit_transform(xbalanced)\n",
    "vec_lda= lda.fit_transform(vec_bow)\n",
    "print(vec_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_importances = LDAmodel.get_document_topics(document, minimum_probability=0)\n",
    "#     topic_importances = np.array(topic_importances)\n",
    "#     return topic_importances[:,1]\n",
    "\n",
    "# train_data['lda_features'] = list(map(lambda doc:\n",
    "#                                       document_to_lda_features(LDAmodel, doc),\n",
    "#                                       train_data.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = np.array(list(map(np.array, vec_lda)))\n",
    "X_w2v = np.array(list(map(np.array, trainDataVecs)))\n",
    "X_combined = np.append(X_lda, X_w2v, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_lda.shape)\n",
    "print(X_w2v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_combined.shape)\n",
    "print(len(ybalanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, ybalanced,stratify=ybalanced, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_train_combined, y_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_xgb_combined= xgb_clf.predict(X_test_combined)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test_w2v, y_predicted_xgb_combined)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_xgb_combined)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Negative','Positive','Neutral'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't seem to do much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may know Recurrent Neural Networks, in a very simplified way, these could be thought of as tradicional neural networks that have loops in them, in a way that allows information to persist, this means they can use information of previous events in the generation of new information <br />\n",
    "We will uses keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "import theano\n",
    "theano.config.optimizer=\"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v.shape\n",
    "#X_train_w2v2= np.array(X_train_w2v,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_w2v, y_train_w2v)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_w2v_lstm = sc.fit_transform(X_train_w2v)\n",
    "X_test_w2v_lstm= sc.transform(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_lstm.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mean squared error as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "model_lstm=Sequential()\n",
    "model_lstm.add(LSTM(output_dim=3,input_shape=(1,150),return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model_lstm.add(LSTM(output_dim=3,input_shape=(1,150),return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model_lstm.add(LSTM(output_dim=3,input_shape=(1,150),return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model_lstm.add(LSTM(output_dim=3,input_shape=(1,150),return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mean_squared_error','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_w2v_categorical=keras.utils.np_utils.to_categorical(y_test_w2v)\n",
    "y_train_w2v_categorical=keras.utils.np_utils.to_categorical(y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_lstm_2= np.reshape(X_train_w2v_lstm,(X_train_w2v_lstm.shape[0],1,X_train_w2v_lstm.shape[1]))\n",
    "X_test_w2v_lstm_2 =np.reshape(X_test_w2v_lstm,(X_test_w2v_lstm.shape[0],1,X_test_w2v_lstm.shape[1]))\n",
    "y_train_w2v_lstm_2= np.reshape(y_train_w2v_categorical,(y_train_w2v_categorical.shape[0],1,y_train_w2v_categorical.shape[1]))\n",
    "y_test_w2v_lstm_2 = np.reshape(y_test_w2v_categorical,(y_test_w2v_categorical.shape[0],1,y_test_w2v_categorical.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_w2v_lstm_2.shape)\n",
    "print(X_test_w2v_lstm_2.shape)\n",
    "print(y_train_w2v_lstm_2.shape)\n",
    "print(y_test_w2v_lstm_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(X_train_w2v_lstm_2, y_train_w2v_lstm_2 ,batch_size=8,verbose=2,epochs=150,validation_data=(X_test_w2v_lstm_2, y_test_w2v_lstm_2))\n",
    "model_lstm.save('LSTM500.h5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result doesn't seem that impressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
